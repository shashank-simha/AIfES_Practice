{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed1f226b-f591-4002-bd67-40d1185c1436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54098593-20ad-42ef-a1c2-9c48aebdbcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,\n",
    "    transform = ToTensor(),\n",
    "    download = True\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = False,\n",
    "    transform = ToTensor(),\n",
    "    download = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f86d46b-4f77-4c46-8da4-5d87969d5c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "015bdb2b-b201-446c-8c5d-152667f1a8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b15b18a-efb4-49a1-a45b-2059e51349bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b350667-383c-4e32-bdf4-de96db5541b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b821961e-28d2-4d5d-91ed-f6e655046ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "276f6180-7d2a-4b0f-98e0-5589b38170ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c282194c-8fca-4920-8dd9-acf0c2344a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loaders = {\n",
    "\n",
    "    'train': DataLoader(train_data,\n",
    "                        batch_size=100,\n",
    "                        shuffle=True,\n",
    "                        num_workers=1),\n",
    "\n",
    "    'test': DataLoader(test_data,\n",
    "                       batch_size=100,\n",
    "                       shuffle=True,\n",
    "                       num_workers=1),\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0c960a5-f6da-4ad2-b06c-b732b6a7b32c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x7ee3eafb0bf0>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x7ee3eafb0d70>}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "486651fa-34aa-4642-95b0-23d0601a6e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ff21058-0b98-4654-96bf-1e9c325e3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1f73c91-1e14-4f32-b2bc-bae3e0e02bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = CNN().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(loaders[\"train\"].dataset)} ({100. * batch_idx / len(loaders[\"train\"]):.0f}%)]\\t{loss.item():.6f}')\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders['test']:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(loaders['test'].dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy {correct}/{len(loaders[\"test\"].dataset)} ({100. * correct /len(loaders[\"test\"].dataset):.0f}%\\f)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ac89de4-e0db-4b75-86ae-86fda1b64da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14628/599564851.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\t2.301694\n",
      "Train Epoch: 1 [2000/60000 (3%)]\t2.290906\n",
      "Train Epoch: 1 [4000/60000 (7%)]\t2.263831\n",
      "Train Epoch: 1 [6000/60000 (10%)]\t2.087738\n",
      "Train Epoch: 1 [8000/60000 (13%)]\t1.963483\n",
      "Train Epoch: 1 [10000/60000 (17%)]\t1.885587\n",
      "Train Epoch: 1 [12000/60000 (20%)]\t1.861990\n",
      "Train Epoch: 1 [14000/60000 (23%)]\t1.877231\n",
      "Train Epoch: 1 [16000/60000 (27%)]\t1.755545\n",
      "Train Epoch: 1 [18000/60000 (30%)]\t1.738193\n",
      "Train Epoch: 1 [20000/60000 (33%)]\t1.874113\n",
      "Train Epoch: 1 [22000/60000 (37%)]\t1.770911\n",
      "Train Epoch: 1 [24000/60000 (40%)]\t1.780739\n",
      "Train Epoch: 1 [26000/60000 (43%)]\t1.749444\n",
      "Train Epoch: 1 [28000/60000 (47%)]\t1.778518\n",
      "Train Epoch: 1 [30000/60000 (50%)]\t1.819260\n",
      "Train Epoch: 1 [32000/60000 (53%)]\t1.784241\n",
      "Train Epoch: 1 [34000/60000 (57%)]\t1.802720\n",
      "Train Epoch: 1 [36000/60000 (60%)]\t1.743663\n",
      "Train Epoch: 1 [38000/60000 (63%)]\t1.730767\n",
      "Train Epoch: 1 [40000/60000 (67%)]\t1.739533\n",
      "Train Epoch: 1 [42000/60000 (70%)]\t1.717921\n",
      "Train Epoch: 1 [44000/60000 (73%)]\t1.691801\n",
      "Train Epoch: 1 [46000/60000 (77%)]\t1.644697\n",
      "Train Epoch: 1 [48000/60000 (80%)]\t1.599917\n",
      "Train Epoch: 1 [50000/60000 (83%)]\t1.629975\n",
      "Train Epoch: 1 [52000/60000 (87%)]\t1.589619\n",
      "Train Epoch: 1 [54000/60000 (90%)]\t1.645636\n",
      "Train Epoch: 1 [56000/60000 (93%)]\t1.621910\n",
      "Train Epoch: 1 [58000/60000 (97%)]\t1.586046\n",
      "\n",
      "Test set: Average loss: 0.0154, Accuracy 9263/10000 (93%\f",
      ")\n",
      "Train Epoch: 2 [0/60000 (0%)]\t1.574401\n",
      "Train Epoch: 2 [2000/60000 (3%)]\t1.578752\n",
      "Train Epoch: 2 [4000/60000 (7%)]\t1.613305\n",
      "Train Epoch: 2 [6000/60000 (10%)]\t1.600178\n",
      "Train Epoch: 2 [8000/60000 (13%)]\t1.617500\n",
      "Train Epoch: 2 [10000/60000 (17%)]\t1.591118\n",
      "Train Epoch: 2 [12000/60000 (20%)]\t1.638232\n",
      "Train Epoch: 2 [14000/60000 (23%)]\t1.566065\n",
      "Train Epoch: 2 [16000/60000 (27%)]\t1.636799\n",
      "Train Epoch: 2 [18000/60000 (30%)]\t1.578244\n",
      "Train Epoch: 2 [20000/60000 (33%)]\t1.620258\n",
      "Train Epoch: 2 [22000/60000 (37%)]\t1.579597\n",
      "Train Epoch: 2 [24000/60000 (40%)]\t1.581002\n",
      "Train Epoch: 2 [26000/60000 (43%)]\t1.608603\n",
      "Train Epoch: 2 [28000/60000 (47%)]\t1.532472\n",
      "Train Epoch: 2 [30000/60000 (50%)]\t1.574811\n",
      "Train Epoch: 2 [32000/60000 (53%)]\t1.556149\n",
      "Train Epoch: 2 [34000/60000 (57%)]\t1.575850\n",
      "Train Epoch: 2 [36000/60000 (60%)]\t1.549549\n",
      "Train Epoch: 2 [38000/60000 (63%)]\t1.647928\n",
      "Train Epoch: 2 [40000/60000 (67%)]\t1.632239\n",
      "Train Epoch: 2 [42000/60000 (70%)]\t1.573335\n",
      "Train Epoch: 2 [44000/60000 (73%)]\t1.553840\n",
      "Train Epoch: 2 [46000/60000 (77%)]\t1.589259\n",
      "Train Epoch: 2 [48000/60000 (80%)]\t1.570288\n",
      "Train Epoch: 2 [50000/60000 (83%)]\t1.590957\n",
      "Train Epoch: 2 [52000/60000 (87%)]\t1.601675\n",
      "Train Epoch: 2 [54000/60000 (90%)]\t1.585799\n",
      "Train Epoch: 2 [56000/60000 (93%)]\t1.542654\n",
      "Train Epoch: 2 [58000/60000 (97%)]\t1.600327\n",
      "\n",
      "Test set: Average loss: 0.0151, Accuracy 9501/10000 (95%\f",
      ")\n",
      "Train Epoch: 3 [0/60000 (0%)]\t1.615407\n",
      "Train Epoch: 3 [2000/60000 (3%)]\t1.584692\n",
      "Train Epoch: 3 [4000/60000 (7%)]\t1.593795\n",
      "Train Epoch: 3 [6000/60000 (10%)]\t1.596504\n",
      "Train Epoch: 3 [8000/60000 (13%)]\t1.554058\n",
      "Train Epoch: 3 [10000/60000 (17%)]\t1.557347\n",
      "Train Epoch: 3 [12000/60000 (20%)]\t1.589218\n",
      "Train Epoch: 3 [14000/60000 (23%)]\t1.593071\n",
      "Train Epoch: 3 [16000/60000 (27%)]\t1.588751\n",
      "Train Epoch: 3 [18000/60000 (30%)]\t1.545722\n",
      "Train Epoch: 3 [20000/60000 (33%)]\t1.556470\n",
      "Train Epoch: 3 [22000/60000 (37%)]\t1.545250\n",
      "Train Epoch: 3 [24000/60000 (40%)]\t1.557011\n",
      "Train Epoch: 3 [26000/60000 (43%)]\t1.595855\n",
      "Train Epoch: 3 [28000/60000 (47%)]\t1.547775\n",
      "Train Epoch: 3 [30000/60000 (50%)]\t1.601676\n",
      "Train Epoch: 3 [32000/60000 (53%)]\t1.552910\n",
      "Train Epoch: 3 [34000/60000 (57%)]\t1.548665\n",
      "Train Epoch: 3 [36000/60000 (60%)]\t1.497673\n",
      "Train Epoch: 3 [38000/60000 (63%)]\t1.587679\n",
      "Train Epoch: 3 [40000/60000 (67%)]\t1.577010\n",
      "Train Epoch: 3 [42000/60000 (70%)]\t1.556710\n",
      "Train Epoch: 3 [44000/60000 (73%)]\t1.527268\n",
      "Train Epoch: 3 [46000/60000 (77%)]\t1.569536\n",
      "Train Epoch: 3 [48000/60000 (80%)]\t1.508968\n",
      "Train Epoch: 3 [50000/60000 (83%)]\t1.529422\n",
      "Train Epoch: 3 [52000/60000 (87%)]\t1.569309\n",
      "Train Epoch: 3 [54000/60000 (90%)]\t1.564947\n",
      "Train Epoch: 3 [56000/60000 (93%)]\t1.577562\n",
      "Train Epoch: 3 [58000/60000 (97%)]\t1.596463\n",
      "\n",
      "Test set: Average loss: 0.0150, Accuracy 9630/10000 (96%\f",
      ")\n",
      "Train Epoch: 4 [0/60000 (0%)]\t1.582318\n",
      "Train Epoch: 4 [2000/60000 (3%)]\t1.542063\n",
      "Train Epoch: 4 [4000/60000 (7%)]\t1.571029\n",
      "Train Epoch: 4 [6000/60000 (10%)]\t1.538180\n",
      "Train Epoch: 4 [8000/60000 (13%)]\t1.579502\n",
      "Train Epoch: 4 [10000/60000 (17%)]\t1.515648\n",
      "Train Epoch: 4 [12000/60000 (20%)]\t1.550153\n",
      "Train Epoch: 4 [14000/60000 (23%)]\t1.596227\n",
      "Train Epoch: 4 [16000/60000 (27%)]\t1.554759\n",
      "Train Epoch: 4 [18000/60000 (30%)]\t1.549716\n",
      "Train Epoch: 4 [20000/60000 (33%)]\t1.550212\n",
      "Train Epoch: 4 [22000/60000 (37%)]\t1.567062\n",
      "Train Epoch: 4 [24000/60000 (40%)]\t1.526590\n",
      "Train Epoch: 4 [26000/60000 (43%)]\t1.521553\n",
      "Train Epoch: 4 [28000/60000 (47%)]\t1.605077\n",
      "Train Epoch: 4 [30000/60000 (50%)]\t1.511064\n",
      "Train Epoch: 4 [32000/60000 (53%)]\t1.510369\n",
      "Train Epoch: 4 [34000/60000 (57%)]\t1.570949\n",
      "Train Epoch: 4 [36000/60000 (60%)]\t1.537012\n",
      "Train Epoch: 4 [38000/60000 (63%)]\t1.537994\n",
      "Train Epoch: 4 [40000/60000 (67%)]\t1.527278\n",
      "Train Epoch: 4 [42000/60000 (70%)]\t1.562292\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m11\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     test()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(epoch)\u001b[39m\n\u001b[32m     16\u001b[39m output = model(data)\n\u001b[32m     17\u001b[39m loss = loss_fn(output, target)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m optimizer.step()\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_idx % \u001b[32m20\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/74C33DD47161591B/TUM/AIfES/venv/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/74C33DD47161591B/TUM/AIfES/venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/74C33DD47161591B/TUM/AIfES/venv/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f388f-7df3-4b42-b02e-656bdc372513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
